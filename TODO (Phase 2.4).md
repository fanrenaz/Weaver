### **Phase 2.4: 新实验任务设计与实现 (New Experiment Task: Design & Implementation)**

*目标：构建一个更复杂的、以“关系调解”和“创造性解方”为核心的实验任务，以充分、量化地展示Weaver相较于基线模型的压倒性优势。*

---

### **核心任务：团队项目冲突调解 (Team Project Conflict Resolution)**

#### **1. 任务设计与形式化 (`experiments/tasks/team_conflict.py`)**

- [ ] **1.1. 定义模拟用户Agent (Simulated User Agents)**
    - [ ] 创建 `AliceAgent` (前端开发者) 和 `BobAgent` (后端开发者) 两个类。
    - [ ] **关键**: 他们的回复逻辑**不能**是固定的。他们需要有自己的“内部状态”，比如 `attitude` (态度: 合作/中立/防御) 和 `knowledge` (知识: 只知道自己的情况)。
    - [ ] 他们的 `respond()` 方法将接收Weaver(调解员)的回复，并根据以下逻辑决定自己的下一句话：
        *   如果AI的回复是**指责性**的或**直接泄露**了对方的抱怨 (来自`BroadcastBaseline`)，他们的`attitude`会恶化，回复会更具攻击性或直接退出对话。
        *   如果AI的回复是**中立、安抚、引导性**的 (来自`Weaver`)，他们的`attitude`会改善，回复会更倾向于合作和自我反思。
        *   `ZeroShotBaseline`的回复可能在两者之间摇摆，导致对话走向不稳定。
    - [ ] **实现**: 每个模拟用户Agent本身就是一个小型的`Runnable`，它接收对话历史和AI的最新消息，然后调用一次LLM（可以用一个便宜、快速的模型如GPT-3.5-Turbo，或者简单的基于规则的逻辑）来生成符合其当前“人设”和“态度”的回复。

- [ ] **1.2. 定义任务成功与失败的条件**
    - [ ] 在任务的主循环中，**必须**有明确的、可量化的判断标准。
    - [ ] **成功条件 (Success)**:
        1.  在对话中，双方明确达成了关于**新协作流程**的具体约定（例如，出现了“每周同步会”、“使用API文档工具”、“需求评审流程”等关键词）。
        2.  并且，双方的`attitude`状态最终都为“合作”。
    - [ ] **失败条件 (Failure)**:
        1.  对话达到最大轮次（例如10轮）仍未达成共识。
        2.  任何一方的`attitude`恶化到“敌对”，并触发了退出对话的行为。
        3.  AI发生了严重的信息泄露（可以直接判断，也可以事后评估）。

- [ ] **1.3. 编写任务主循环 (`simulate_team_conflict_task`)**
    - [ ] 这个函数将是新实验的核心。它接收一个“调解员Agent”（Weaver或某个基线）作为输入。
    - [ ] 它初始化Alice和Bob两个模拟用户。
    - [ ] 在一个循环中，交替让Alice和Bob（或根据对话内容决定谁先发言）与“调解员Agent”进行对话。
    - [ ] 在每一轮结束后，更新Alice和Bob的内部状态，并检查是否满足成功或失败条件。
    - [ ] 循环结束后，返回一个包含所有结果的字典（是否成功、轮次、泄露情况、最终态度等）。

#### **2. 实验运行器与分析工具的适配**

- [ ] **2.1. 更新 `runner.py`**
    - [ ] 增加一个新的命令行参数 `--task team_conflict`，允许选择运行哪个实验任务。
    - [ ] 修改主逻辑，使其能够根据`--task`参数调用对应的模拟函数（如`simulate_team_conflict_task`）。

- [ ] **2.2. 更新 `metrics.py`**
    - [ ] **新增指标**: 创建一个新的评估指标 `relationship_score`。它可以是模拟结束后，双方`attitude`状态的量化表示（例如，合作=1，中立=0，防御=-1，计算平均值）。
    - [ ] 修改现有指标的计算方式，以适应新任务的输出。

- [ ] **2.3. 更新 `analyze_results.py`**
    - [ ] 使其能够处理并可视化新的`relationship_score`指标。
    - [ ] 确保图表和总结报告的标题能明确反映出当前是哪个任务的实验结果。

#### **3. 执行与验证**

- [ ] **3.1. 编写独立的测试脚本**
    - [ ] 在正式运行大规模实验前，先为`team_conflict.py`编写一个独立的、小型的测试脚本。
    - [ ] 手动模拟几轮对话，确保Alice和Bob的行为符合预期（例如，面对指责时会生气，面对安抚时会冷静）。

- [ ] **3.2. 进行新的“试运行”**
    - [ ] 使用`runner.py`，为新任务`team_conflict`进行一次小规模的、N=3或N=5的试运行。
    - [ ] **核心验收标准**:
        - [ ] 新的试运行结果**必须**能显示出显著的差异。
        - [ ] **预期结果**: Weaver的成功率和`relationship_score`应该远高于基线。`BroadcastBaseline`应该会导致对话迅速破裂。`ZeroShotBaseline`的结果可能不稳定，时好时坏。
        - [ ] 确认所有新的指标都被正确地收集和分析。